{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lElF3o5PR6ys"
   },
   "source": [
    "# Your First RAG Application: Stone Ridge Investor Letter Assistant\n",
    "\n",
    "> **Note:** While the examples in this notebook use the OpenAI API, please follow the best practices outlined in the [SRHG AI Usage Guidelines](https://srhg.enterprise.slack.com/docs/T0HANKTEC/F0AB86J3A1L).\n",
    "\n",
    "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application by building a **Stone Ridge Investor Letter Assistant**.\n",
    "\n",
    "Imagine having an AI assistant that can answer your questions about Stone Ridge's investment philosophy, market insights, and strategic outlook based on their investor letters - that's exactly what we'll build here. We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
    "\n",
    "> NOTE: This was done with Python 3.12.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CtcL8P8R6yt"
   },
   "source": [
    "## Table of Contents:\n",
    "\n",
    "- Task 1: Imports and Utilities\n",
    "- Task 2: Documents (Loading the Stone Ridge Investor Letter)\n",
    "- Task 3: Embeddings and Vectors\n",
    "- Task 4: Prompts\n",
    "- Task 5: Retrieval Augmented Generation (Building the Investor Letter Assistant)\n",
    "  - ðŸš§ Activity #1: Enhance Your Investor Letter Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Dz6GYilR6yt"
   },
   "source": [
    "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
    "\n",
    "<img src=\"https://i.imgur.com/vD8b016.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjmC0KFtR6yt"
   },
   "source": [
    "## Task 1: Imports and Utilities\n",
    "\n",
    "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z1dyrG4hR6yt"
   },
   "outputs": [],
   "source": [
    "from aimakerspace.text_utils import PDFFileLoader, CharacterTextSplitter\n",
    "from aimakerspace.vectordatabase import VectorDatabase\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9OrFZRnER6yt"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0jGnpQsR6yu"
   },
   "source": [
    "## Task 2: Documents\n",
    "\n",
    "We'll be concerning ourselves with this part of the flow in the following section:\n",
    "\n",
    "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SFPWvRUR6yu"
   },
   "source": [
    "### Loading Source Documents\n",
    "\n",
    "So, first things first, we need some documents to work with.\n",
    "\n",
    "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
    "\n",
    "In this case, we're going to parse our PDF file into a single document in memory.\n",
    "\n",
    "Let's look at the relevant bits of the `PDFFileLoader` class:\n",
    "\n",
    "```python\n",
    "def load_file(self):\n",
    "        doc = pymupdf.open(self.path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        self.documents.append(text)\n",
    "```\n",
    "\n",
    "We're loading the PDF document using PyMuPDF, extracting text from each page, and storing that output in our `self.documents` list.\n",
    "\n",
    "> NOTE: We're using the Stone Ridge 2025 Investor Letter as our sample data. This content covers investment philosophy, market analysis, and strategic insights - perfect for building an investor letter assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ia2sUEuGR6yu",
    "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader = PDFFileLoader(\"data/Stone Ridge 2025 Investor Letter.pdf\")\n",
    "documents = pdf_loader.load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV-tj5WFR6yu",
    "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025 Investor Letter\n",
      "Investor Letter\n",
      "â€œEvery driver has a limit.  Mine is a little bit further than o\n"
     ]
    }
   ],
   "source": [
    "print(documents[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHlTvCzYR6yu"
   },
   "source": [
    "### Splitting Text Into Chunks\n",
    "\n",
    "As we can see, there is one massive document.\n",
    "\n",
    "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
    "\n",
    "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
    "\n",
    "For this toy example, we'll just split blindly on length.\n",
    "\n",
    ">There's an opportunity to clear up some terminology here, for this course we will stick to the following:\n",
    ">\n",
    ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
    ">- \"document(s)\" : single (or more) text object(s)\n",
    ">- \"corpus\" : the combination of all of our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6Voc0jR6yv"
   },
   "source": [
    "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into manageable sized chunks that retain the most relevant local context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMC4tsEmR6yv",
    "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "split_documents = text_splitter.split_texts(documents)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a section to tag whats endnotes and whats not\n",
    "new_split_docs = []\n",
    "section = 'body'\n",
    "for doc in split_documents:\n",
    "    if 'Endnotes' in doc:\n",
    "        pre, post = doc.split('Endnotes')\n",
    "        if len(pre):\n",
    "            new_split_docs.append((pre, 'body'))\n",
    "        if len(post):\n",
    "            new_split_docs.append((post, 'endnotes'))\n",
    "\n",
    "        section = 'endnotes'\n",
    "    else:\n",
    "        new_split_docs.append((doc, section))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2wKT0WLR6yv"
   },
   "source": [
    "Let's take a look at some of the documents we've managed to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcYMwWJoR6yv",
    "outputId": "20d69876-feca-4826-b4be-32915276987a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2025 Investor Letter\\nInvestor Letter\\nâ€œEvery driver has a limit.  Mine is a little bit further than others.â€\\nâ€”\\u2002 Ayrton Senna, greatest Formula One driver of all time\\nâ€œIâ€™m not funny.  What I am is brave.â€\\nâ€”\\u2002 Lucille Ball, greatest female comedian of all time\\nâ€œIâ€™d rather be optimistic and wrong than pessimistic and right.â€\\nâ€”\\u2002 Elon Musk\\nâ€œI can outlearn you.  \\nI can outread you.  \\nI can outthink you.  \\nAnd I can outphilosophize you.â€\\nâ€”\\u2002 Robert De Niro as Max Cady, Cape Fear\\nâ€œIâ€™m not superstitious, but I am a little stitious.â€\\nâ€”\\u2002 Michael Scott, The Office, on how Bayesians form priors \\nâ€œA wall is a very big weapon.â€\\nâ€”\\u2002 Banksy\\nDecember 2025\\nDear Fellow Investor,\\nThe greatest treasure hunters know that the treasure they seek is not lost.  It is waiting.\\nStarting in 1969 and for 5,954 straight days, treasure hunter Mel Fisher sought what was waiting for him: the \\nwreckage of the Nuestra SeÃ±ora de Atocha, a Spanish galleon that sunk in 1622.  \\nHow do you recruit, retain, and motivate a crew when',\n",
       "  'body')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_split_docs[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tion set \\nforth herein. By accepting this communication, the recipient acknowledges its understanding and acceptance of the foregoing \\nterms.\\n',\n",
       "  'endnotes')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_split_docs[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOU-RFP_R6yv"
   },
   "source": [
    "## Task 3: Embeddings and Vectors\n",
    "\n",
    "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
    "\n",
    "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API Key\n",
    "\n",
    "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
    "\n",
    "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "\n",
    "Let's set up our vector database to hold all our documents and their embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDQrfAR1R6yv"
   },
   "source": [
    "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
    "\n",
    "Let's look at our `VectorDatabase().__init__()`:\n",
    "\n",
    "```python\n",
    "def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors = defaultdict(np.array)\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "```\n",
    "\n",
    "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
    "\n",
    "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
    "\n",
    "> **Quick Info About `text-embedding-3-small`**:\n",
    "> - It has a context window of **8191** tokens\n",
    "> - It returns vectors with dimension **1536**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L273pRdeR6yv"
   },
   "source": [
    "#### â“Question #1:\n",
    "\n",
    "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
    "\n",
    "1. Is there any way to modify this dimension?\n",
    "2. What technique does OpenAI use to achieve this?\n",
    "\n",
    "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1.1, and [this paper](https://arxiv.org/abs/2205.13147) for an answer to question #1.2!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "1. You can pass in a different value for the \"dimensions\" parameter to the request body\n",
    "2. Open AI uses a method called Matryoshka Reprsentation Learning where the embeddings created are increasing in granularity. The idea being if you took the first N dimensions, that should be comparable to embedding in N dimensions, each incremental dimension adds to the granularity of the embedding but does not reflect a change to its previous embeddings. This is sort of like a decimal, where you can truncate the last n digits without losing any of the information in the first n digits. By including more digits, or dimensions, you only get to a more granular representation of the information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5FZY7K3R6yv"
   },
   "source": [
    "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
    "\n",
    "```python\n",
    "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        return await aget_embeddings(\n",
    "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSct6X0aR6yv"
   },
   "source": [
    "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
    "\n",
    "```python\n",
    "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
    "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
    "        for text, embedding in zip(list_of_text, embeddings):\n",
    "            self.insert(text, np.array(embedding))\n",
    "        return self\n",
    "```\n",
    "\n",
    "And that's all we need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "O4KoLbVDR6yv"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aee33b31a8b492cba6fad1051da70f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from aimakerspace.vectordatabase import *\n",
    "\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(list_of_text)\n",
    "        return list(embeddings)\n",
    "\n",
    "    async def async_get_embedding(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(list_of_text)\n",
    "        return list(embeddings)\n",
    "\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text)\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors = defaultdict(np.array)\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "\n",
    "    def insert(self, key: str, vector: np.array) -> None:\n",
    "        self.vectors[key] = vector\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query_vector: np.array,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        scores = [\n",
    "            (key, distance_measure(query_vector, vector))\n",
    "            for key, vector in self.vectors.items()\n",
    "        ]\n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    def search_by_text(\n",
    "        self,\n",
    "        query_text: str,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "        return_as_text: bool = False,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        query_vector = self.embedding_model.get_embedding(query_text)\n",
    "        results = self.search(query_vector, k, distance_measure)\n",
    "        return [result[0] for result in results] if return_as_text else results\n",
    "\n",
    "    def retrieve_from_key(self, key: str) -> np.array:\n",
    "        return self.vectors.get(key, None)\n",
    "\n",
    "    async def abuild_from_list(self, list_of_text: List[Tuple[str, str]]) -> \"VectorDatabase\":\n",
    "        embeddings = await self.embedding_model.async_get_embeddings([t[0] for t in list_of_text])\n",
    "        for (text, metadata), embedding in zip(list_of_text, embeddings):\n",
    "            if metadata == 'body':\n",
    "                self.insert(text, np.array(embedding))\n",
    "        return self\n",
    "\n",
    "em = EmbeddingModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = VectorDatabase(em)\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(new_split_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSZwaGvpR6yv"
   },
   "source": [
    "#### â“Question #2:\n",
    "\n",
    "What are the benefits of using an `async` approach to collecting our embeddings?\n",
    "\n",
    "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "Since this is presumably IO bound, using async means you can send all your requests in one shot (or a few batches) and wait for the responses, this allows us to lean on any horizontal scaling in the openai embedding api. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRBdIt-xR6yw"
   },
   "source": [
    "So, to review what we've done so far in natural language:\n",
    "\n",
    "1. We load source documents\n",
    "2. We split those source documents into smaller chunks (documents)\n",
    "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
    "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-vWANZyR6yw"
   },
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
    "\n",
    "We're going to use the following process to achieve this in our toy example:\n",
    "\n",
    "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
    "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
    "3. We return a list of the top `k` closest vectors, with their text representations\n",
    "\n",
    "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
    "\n",
    "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
    "\n",
    "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hen we believe \\nit is time â€“ have been the hallmarks of our operating model since firm inception. \\nThe first rule of product design at Stone Ridge is we build products we want for ourselves.  Stone Ridge is always \\nthe seed investor and, whenever possible â€“ but not yet always â€“ the largest.  Consider our â€œno-deviceâ€ policy in \\nthis context.  Would you ever pull out your phone and start scrolling in the middle of a meeting with your largest \\ninvestor?  At Stone Ridge, we do not pull our phones out on each other.\\nWith Stone Ridge increasingly Stone Ridgeâ€™s largest investor, the scale of our profits as principals, not agents, creates \\na level of alignment with our outside investors we are unaware exists elsewhere.  Those investors occasionally \\nremark that this is a wonderfully clarifying and palpable distinction, and that Stone Ridge feels different inside. \\nEarlier this year, Stone Ridge Holdings Group (SRHG), parent company of Stone Ridge Asset Management, NYDIG, \\nand Longtail Re, comp',\n",
       "  np.float32(0.62319523)),\n",
       " ('ave received 32x their original investment in distributions and now own equity \\nvalued up 178x net of those distributions, an annualized total return of 50%.  Fisher took 5,954 days to find the \\nAtocha.  We have been at it for 4,930.  Every morning, I bang our little shipâ€™s pots and pans with a smile because \\nI know Stone Ridge will find the treasure. \\nTodayâ€™s the day.\\n*****\\nOUR PARTNERSHIP\\nAs we enter 2026, our tanks are filled with energy, gratitude, and inspiration.  We innovate to prepare for an \\nuncertain future, in pursuit of our mission: financial security for all.\\nStone Ridge is most proud of the partnership we have with you, our investors.  We are on the path together.  You \\ncontribute the capital to propel and sustain groundbreaking product development.  We contribute our collective \\ncareersâ€™ worth of experience in sourcing, structuring, execution, and risk management.  Together, it works.  In that \\nspirit, I offer my deepest gratitude to you for sharing your wealth with us t',\n",
       "  np.float32(0.56189543)),\n",
       " (' palpable distinction, and that Stone Ridge feels different inside. \\nEarlier this year, Stone Ridge Holdings Group (SRHG), parent company of Stone Ridge Asset Management, NYDIG, \\nand Longtail Re, completed a significant primary equity financing led by an iconic American financial services firm \\nâ€“ also a mutual, though this one a relatively spry 165-year-old â€“ we are thrilled to partner with in the decades ahead. \\nDespite regularly rising profitability, regularly raising primary equity for our parent and affiliates â€“ $4.6 billion \\nand counting, and only primary, ever â€“ has been our quiet way of strengthening our most important partnerships, \\nplaying muscular defense when the sky falls in our markets, and playing profitable post-event offense in the \\naftermath.\\nFounding 2012 SRHG investors have received 32x their original investment in distributions and now own equity \\nvalued up 178x net of those distributions, an annualized total return of 50%.  Fisher took 5,954 days to find the \\nAtoch',\n",
       "  np.float32(0.5543321))]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.search_by_text(\"What is Stone Ridge's investment philosophy?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TehsfIiKR6yw"
   },
   "source": [
    "## Task 4: Prompts\n",
    "\n",
    "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
    "\n",
    "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
    "\n",
    "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXpA0UveR6yw"
   },
   "source": [
    "### XYZRolePrompt\n",
    "\n",
    "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
    "\n",
    "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
    "\n",
    "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
    "\n",
    "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the modelâ€™s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
    "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
    "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
    "\n",
    "The main idea is this:\n",
    "\n",
    "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
    "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
    "3. Then, you prompt the model with the true \"user\" message.\n",
    "\n",
    "In this example, we'll be forgoing the 2nd step for simplicity's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdZ2KWKSR6yw"
   },
   "source": [
    "#### Utility Functions\n",
    "\n",
    "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFbeJDDsR6yw"
   },
   "source": [
    "##### XYZRolePrompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mojJSE3R6yw"
   },
   "source": [
    "Here we have our `system`, `user`, and `assistant` role prompts.\n",
    "\n",
    "Let's take a peek at what they look like:\n",
    "\n",
    "```python\n",
    "class BasePrompt:\n",
    "    def __init__(self, prompt):\n",
    "        \"\"\"\n",
    "        Initializes the BasePrompt object with a prompt template.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        \"\"\"\n",
    "        self.prompt = prompt\n",
    "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
    "\n",
    "    def format_prompt(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Formats the prompt string using the keyword arguments provided.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: The formatted prompt string\n",
    "        \"\"\"\n",
    "        matches = self._pattern.findall(self.prompt)\n",
    "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
    "\n",
    "    def get_input_variables(self):\n",
    "        \"\"\"\n",
    "        Gets the list of input variable names from the prompt string.\n",
    "\n",
    "        :return: List of input variable names\n",
    "        \"\"\"\n",
    "        return self._pattern.findall(self.prompt)\n",
    "```\n",
    "\n",
    "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
    "\n",
    "```python\n",
    "class RolePrompt(BasePrompt):\n",
    "    def __init__(self, prompt, role: str):\n",
    "        \"\"\"\n",
    "        Initializes the RolePrompt object with a prompt template and a role.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
    "        \"\"\"\n",
    "        super().__init__(prompt)\n",
    "        self.role = role\n",
    "\n",
    "    def create_message(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a message dictionary with a role and a formatted message.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: Dictionary containing the role and the formatted message\n",
    "        \"\"\"\n",
    "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
    "```\n",
    "\n",
    "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
    "\n",
    "```python\n",
    "class SystemRolePrompt(RolePrompt):\n",
    "    def __init__(self, prompt: str):\n",
    "        super().__init__(prompt, \"system\")\n",
    "```\n",
    "\n",
    "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D361R6sMR6yw"
   },
   "source": [
    "##### ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJVQ2Pm8R6yw"
   },
   "source": [
    "Next we have our model, which is converted to a format analogous to libraries like LangChain and LlamaIndex.\n",
    "\n",
    "Let's take a peek at how that is constructed:\n",
    "\n",
    "```python\n",
    "class ChatOpenAI:\n",
    "    def __init__(self, model_name: str = \"gpt-4.1-mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if self.openai_api_key is None:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "    def run(self, messages, text_only: bool = True):\n",
    "        if not isinstance(messages, list):\n",
    "            raise ValueError(\"messages must be a list\")\n",
    "\n",
    "        openai.api_key = self.openai_api_key\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name, messages=messages\n",
    "        )\n",
    "\n",
    "        if text_only:\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        return response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCU7FfhIR6yw"
   },
   "source": [
    "#### â“ Question #3:\n",
    "\n",
    "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
    "\n",
    "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "You can use the instructions api to pass in instructions that you may otherwise put in the developer role message. I guess this is theoretically useful if you want to keep a conversation going and only want to apply instructions to certain messages, i wonder if you removed the developer message on those conversation lines if it would confuse the llm. Not clear how useful this is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5wcjMLCR6yw"
   },
   "source": [
    "### Creating and Prompting OpenAI's `gpt-4.1-mini`!\n",
    "\n",
    "Let's tie all these together and use it to prompt `gpt-4.1-mini`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "WIfpIot7R6yw"
   },
   "outputs": [],
   "source": [
    "from aimakerspace.openai_utils.prompts import (\n",
    "    UserRolePrompt,\n",
    "    SystemRolePrompt,\n",
    "    AssistantRolePrompt,\n",
    ")\n",
    "\n",
    "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
    "\n",
    "chat_openai = ChatOpenAI()\n",
    "user_prompt_template = \"{content}\"\n",
    "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
    "system_prompt_template = (\n",
    "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
    ")\n",
    "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
    "\n",
    "messages = [\n",
    "    system_role_prompt.create_message(expertise=\"Python\"),\n",
    "    user_role_prompt.create_message(\n",
    "        content=\"What is the best way to write a loop?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = chat_openai.run(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHo7lssNR6yw",
    "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! The \"best\" way to write a loop in Python depends on what you're trying to achieve, but generally, Python's `for` loops and `while` loops are both very useful and have their ideal use cases.\n",
      "\n",
      "Hereâ€™s a brief overview to help you write effective loops:\n",
      "\n",
      "### Using a `for` loop\n",
      "`for` loops are great when you want to iterate over a sequence (like a list, tuple, string, or range).\n",
      "\n",
      "Example:\n",
      "```python\n",
      "# Looping through a list\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "\n",
      "If you want to loop a specific number of times:\n",
      "```python\n",
      "for i in range(5):\n",
      "    print(i)\n",
      "```\n",
      "\n",
      "### Using a `while` loop\n",
      "`while` loops are useful when you want to repeat something until a condition changes.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "count = 0\n",
      "while count < 5:\n",
      "    print(count)\n",
      "    count += 1\n",
      "```\n",
      "\n",
      "### Best Practices\n",
      "- Prefer `for` loops when iterating over collections or a known number of iterations.\n",
      "- Use `while` loops when the number of repetitions depends on a condition changing within the loop.\n",
      "- Make sure to update the loop variable or condition within `while` loops to avoid infinite loops.\n",
      "- Keep the loop body as simple as possible.\n",
      "- Use descriptive variable names for readability.\n",
      "\n",
      "If you have a specific task or type of loop in mind, feel free to share it, and Iâ€™d be happy to help you write it! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2nxxhB2R6yy"
   },
   "source": [
    "## Task 5: Retrieval Augmented Generation\n",
    "\n",
    "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
    "\n",
    "There is much you could do here, many tweaks and improvements to be made!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "D1hamzGaR6yy"
   },
   "outputs": [],
   "source": [
    "RAG_SYSTEM_TEMPLATE = \"\"\"You are a helpful investor letter assistant that answers questions about Stone Ridge's investment philosophy, market insights, and strategic outlook based strictly on provided context.\n",
    "\n",
    "Instructions:\n",
    "- Only answer questions using information from the provided context\n",
    "- If the context doesn't contain relevant information, respond with \"I don't have information about that in the investor letter\"\n",
    "- Be accurate and cite specific parts of the context when possible\n",
    "- Keep responses {response_style} and {response_length}\n",
    "- Only use the provided context. Do not use external knowledge.\n",
    "- Include a reminder that this is for informational purposes only and not investment advice when appropriate\n",
    "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
    "\n",
    "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
    "{context}\n",
    "\n",
    "Number of relevant sources found: {context_count}\n",
    "{similarity_scores}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Please provide your answer based solely on the context above.\"\"\"\n",
    "\n",
    "rag_system_prompt = SystemRolePrompt(\n",
    "    RAG_SYSTEM_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"response_style\": \"concise\",\n",
    "        \"response_length\": \"brief\"\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_user_prompt = UserRolePrompt(\n",
    "    RAG_USER_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"context_count\": \"\",\n",
    "        \"similarity_scores\": \"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI, vector_db_retriever: VectorDatabase, \n",
    "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.response_style = response_style\n",
    "        self.include_scores = include_scores\n",
    "\n",
    "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
    "        # Retrieve relevant contexts\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
    "        \n",
    "        context_prompt = \"\"\n",
    "        similarity_scores = []\n",
    "        \n",
    "        for i, (context, score) in enumerate(context_list, 1):\n",
    "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
    "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
    "        \n",
    "        # Create system message with parameters\n",
    "        system_params = {\n",
    "            \"response_style\": self.response_style,\n",
    "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
    "        }\n",
    "        \n",
    "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
    "        \n",
    "        user_params = {\n",
    "            \"user_query\": user_query,\n",
    "            \"context\": context_prompt.strip(),\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
    "        }\n",
    "        \n",
    "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
    "\n",
    "        return {\n",
    "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
    "            \"context\": context_list,\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
    "            \"prompts_used\": {\n",
    "                \"system\": formatted_system_prompt,\n",
    "                \"user\": formatted_user_prompt\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the provided excerpts from the investor letter, several key investment themes emerge:\n",
      "\n",
      "1. **Pursuit of Competitive Advantage Through Sophistication and Operational Excellence**  \n",
      "   The letter discusses Stone Ridge's initial belief that breakthroughs in proprietary financing and hedging technology, resulting in an edge in cost of funds, would suffice for best-in-class investment performance in energy markets (Source 1). However, they learned that financial sophistication alone is insufficient without industrial control and operational expertise. This led them to partner with Flywheel Energy, recognized for superior operations partly due to their leadership's elite military backgrounds. This highlights a theme of combining financial innovation with operational mastery to achieve superior investment outcomes.\n",
      "\n",
      "2. **Long-Term Perseverance and Search for Value**  \n",
      "   Stone Ridge draws an analogy to the long search for the treasure of the Atocha shipwreck, emphasizing patience and persistence over thousands of days (Source 2). This metaphor reflects a commitment to disciplined, long-term investing with a focus on discovering undervalued opportunities that might require sustained effort.\n",
      "\n",
      "3. **Partnership and Shared Mission with Investors**  \n",
      "   The firm expresses gratitude toward its investors and emphasizes the partnership ethos: investors provide capital to fuel innovation and product development, while Stone Ridge contributes deep expertise in sourcing, structuring, execution, and risk management (Source 2). This theme showcases a collaborative approach where aligned incentives and shared dedication to financial security underpin their operations.\n",
      "\n",
      "4. **Risk Management and Selective Investment Approach in Complex Markets**  \n",
      "   In their reinsurance business (Longtail Re), initiated in 2020, Stone Ridge focuses on partnering with top underwriters to build a hyper-diversified portfolio of casualty liabilities that generate low-cost float, which is then deployed into proprietary fixed income strategies (Source 3). They are cautious of adverse selection and highlight the importance of selectivity and avoiding superficially attractive but risky trades. The firm underscores its philosophy of prudent risk transfer pricing and strategic partnerships rather than competing with established legacy underwriters.\n",
      "\n",
      "5. **Innovation Amidst Uncertainty**  \n",
      "   The letter communicates a forward-looking theme of innovating to prepare for an uncertain future and pursuing the mission of financial security for all (Source 2). This suggests a broad investment philosophy oriented towards adaptive strategies and continuous product development in the face of changing market and economic conditions.\n",
      "\n",
      "In summary, the key investment themes include leveraging proprietary financial and operational advantages, long-term value discovery with persistence, strong partnership alignment with investors, disciplined and selective risk management especially in insurance-related markets, and ongoing innovation to address uncertainty and secure financial outcomes.\n",
      "\n",
      "**Note:** This response is for informational purposes only and does not constitute investment advice.\n",
      "\n",
      "Context Count: 3\n",
      "Similarity Scores: ['Source 1: 0.434', 'Source 2: 0.432', 'Source 3: 0.410']\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai,\n",
    "    response_style=\"detailed\",\n",
    "    include_scores=True\n",
    ")\n",
    "\n",
    "result = rag_pipeline.run_pipeline(\n",
    "    \"What are the key investment themes discussed in the letter?\",\n",
    "    k=3,\n",
    "    response_length=\"comprehensive\", \n",
    "    include_warnings=True,\n",
    "    confidence_required=True\n",
    ")\n",
    "\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"\\nContext Count: {result['context_count']}\")\n",
    "print(f\"Similarity Scores: {result['similarity_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZIJI19uR6yz"
   },
   "source": [
    "#### â“ Question #4:\n",
    "\n",
    "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
    "\n",
    "What is that strategy called?\n",
    "\n",
    "> NOTE: You can look through our [OpenAI Responses API](https://colab.research.google.com/drive/14SCfRnp39N7aoOx8ZxadWb0hAqk4lQdL?usp=sharing) notebook for an answer to this question if you get stuck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "You could jack up the reasoning effort, the other thing would be using the developer role, but its not clear to me how this is different than the system role - they seem to be somwhat interchangeable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—ï¸ Activity #1:\n",
    "\n",
    "Enhance your Stone Ridge Investor Letter Assistant in some way! \n",
    "\n",
    "Suggestions are: \n",
    "\n",
    "- **Multi-Document Support**: Allow it to ingest multiple investor letters from different years for historical comparison\n",
    "- **New Distance Metric**: Implement a different similarity measure - does it improve retrieval for financial content?\n",
    "- **Metadata Support**: Add metadata like year, topic categories (market outlook, strategy, performance) to help filter results\n",
    "- **Different Embedding Model**: Try a different embedding model - does domain-specific tuning help for financial content?\n",
    "- **Multi-Source Ingestion**: Add the capability to ingest content from SEC filings, earnings calls, or other financial documents\n",
    "\n",
    "While these are suggestions, you should feel free to make whatever augmentations you desire! Think about what features would make this investor letter assistant most useful for understanding Stone Ridge's investment approach.\n",
    "\n",
    "When you're finished making the augments to your RAG application - vibe check it against the old one - see if you can \"feel the improvement\"!\n",
    "\n",
    "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!\n",
    "\n",
    "> NOTE: If you're not sure where to start - ask Cursor (CMD/CTRL+L) to guide you through the changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/2023 Stone Ridge Investor Letter No Front Image.pdf 2023\n",
      "2023 60\n",
      "./data/Stone_Ridge_2024_Investor_Letter.pdf 2024\n",
      "2024 86\n",
      "./data/2020 Stone Ridge Investor Letter No Front Image.pdf 2020\n",
      "2020 75\n",
      "./data/Stone Ridge 2025 Investor Letter.pdf 2025\n",
      "2025 56\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "docs_by_year = {}\n",
    "for fname in os.listdir('./data'):\n",
    "    year = str(int(re.findall(r'\\d+', fname)[0]))\n",
    "    print(os.path.join('./data', fname), year)\n",
    "    pdf_loader = PDFFileLoader(os.path.join('./data', fname))\n",
    "    documents = pdf_loader.load_documents()\n",
    "    assert len(documents) == 1\n",
    "    \n",
    "    assert 'Endnotes' in documents[0]\n",
    "    relevant_doc = documents[0].split('Endnotes')[0]\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    split_documents = text_splitter.split_texts([relevant_doc])\n",
    "    docs_by_year[year] = split_documents\n",
    "    print(year, len(split_documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76556440cb9e44a8b8702d61b4d19449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from aimakerspace.vectordatabase import *\n",
    "\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(list_of_text)\n",
    "        return list(embeddings)\n",
    "\n",
    "    async def async_get_embedding(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(list_of_text)\n",
    "        return list(embeddings)\n",
    "\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text)\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors_by_year = defaultdict(lambda: defaultdict(np.array))\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "\n",
    "    def insert(self, year: str, key: str, vector: np.array) -> None:\n",
    "        self.vectors_by_year[year][key] = vector\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query_vector: np.array,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "        year = None\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        scores = [\n",
    "            (year, key, distance_measure(query_vector, vector))\n",
    "            for year, key_vector in self.vectors_by_year.items() for key, vector in key_vector.items()\n",
    "        ]\n",
    "        if year is not None:\n",
    "            scores = [a for a in scores if a[0] == year]\n",
    "        return sorted(scores, key=lambda x: x[2], reverse=True)[:k]\n",
    "\n",
    "    def search_by_text(\n",
    "        self,\n",
    "        query_text: str,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "        return_as_text: bool = False,\n",
    "        year = None,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        query_vector = self.embedding_model.get_embedding(query_text)\n",
    "        results = self.search(query_vector, k, distance_measure, year=year)\n",
    "        return [result[1] for result in results] if return_as_text else results\n",
    "\n",
    "    def retrieve_from_key(self, year, key: str) -> np.array:\n",
    "        return self.vectors.get(year, {}).get(key, None)\n",
    "\n",
    "    async def abuild_from_list(self, list_of_text_by_year: Dict[str, List[str]]) -> \"VectorDatabase\":\n",
    "        for year, list_of_text in list_of_text_by_year.items():\n",
    "            embeddings = await self.embedding_model.async_get_embeddings([t[0] for t in list_of_text])\n",
    "            for text, embedding in zip(list_of_text, embeddings):\n",
    "                self.insert(year, text, np.array(embedding))\n",
    "        return self\n",
    "\n",
    "em = EmbeddingModel()\n",
    "vector_db = VectorDatabase(em)\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(docs_by_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "processing failed: failed to parse request: invalid type: string \"hi\", expected struct RequestMessage at line 1 column 17",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalServerError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[157]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhi\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agenticai/AIE-cohort-1/02_Dense_Vector_Retrieval/aimakerspace/openai_utils/chatmodel.py:20\u001b[39m, in \u001b[36mChatOpenAI.run\u001b[39m\u001b[34m(self, messages, text_only, **kwargs)\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmessages must be a list\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m client = OpenAI()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_only:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agenticai/AIE-cohort-1/02_Dense_Vector_Retrieval/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agenticai/AIE-cohort-1/02_Dense_Vector_Retrieval/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agenticai/AIE-cohort-1/02_Dense_Vector_Retrieval/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agenticai/AIE-cohort-1/02_Dense_Vector_Retrieval/.venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mInternalServerError\u001b[39m: processing failed: failed to parse request: invalid type: string \"hi\", expected struct RequestMessage at line 1 column 17"
     ]
    }
   ],
   "source": [
    "ChatOpenAI().run(['hi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Responses.create() got an unexpected keyword argument 'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[172]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     date: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m     33\u001b[39m     participants: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdeveloper\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mI have a vector db of stoneridge investor letters from the following years \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvectors_by_year\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m based on teh following user question, what years should i query and what should i query with\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwhat were the key highlights of the 2020 investor letter\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# output_config=\"format\": {\u001b[39;49;00m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#         \"type\": \"json_schema\",\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#         \"schema\": {\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#             \"type\": \"object\",\u001b[39;49;00m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#             \"properties\": {\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#                 \"name\": {\"type\": \"string\"},\u001b[39;49;00m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#                 \"email\": {\"type\": \"string\"},\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#                 \"plan_interest\": {\"type\": \"string\"},\u001b[39;49;00m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#                 \"demo_requested\": {\"type\": \"boolean\"}\u001b[39;49;00m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#             },\u001b[39;49;00m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#             \"required\": [\"name\", \"email\", \"plan_interest\", \"demo_requested\"],\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#             \"additionalProperties\": False\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#         }\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     }\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# event = response.output_parsed\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Responses.create() got an unexpected keyword argument 'messages'"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "class QueryResponseGetter(BaseModel):\n",
    "    years_to_query: List[str]\n",
    "    query: str\n",
    "\n",
    "# response = client.responses.parse(\n",
    "#     model='gpt-4.1-mini',\n",
    "#     input=[\n",
    "#         {\n",
    "#             'role': 'developer',\n",
    "#             'content': f'I have a vector db of stoneridge investor letters from the following years {\", \".join(vector_db.vectors_by_year.keys())} based on teh following user question, what years should i query and what should i query with'\n",
    "#         },\n",
    "#         {\n",
    "#             'role': 'user',\n",
    "#             'content': 'what were the key highlights of the 2020 investor letter'\n",
    "#         }\n",
    "#     ],\n",
    "#     text_format=QueryResponseGetter\n",
    "# )\n",
    "model='gpt-4.1-mini'\n",
    "# model='gpt-5'\n",
    "context = [\n",
    "  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "  { \"role\": \"user\", \"content\": \"Hello!\" }\n",
    "]\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=model,\n",
    "    input=[\n",
    "        {\n",
    "            'role': 'developer',\n",
    "            'content': f'I have a vector db of stoneridge investor letters from the following years {\", \".join(vector_db.vectors_by_year.keys())} based on the following user question, what years should i query and what should i query with'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'what were the key highlights of the 2020 investor letter'\n",
    "        }\n",
    "    ],\n",
    "    # output_config=\"format\": {\n",
    "    #         \"type\": \"json_schema\",\n",
    "    #         \"schema\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"name\": {\"type\": \"string\"},\n",
    "    #                 \"email\": {\"type\": \"string\"},\n",
    "    #                 \"plan_interest\": {\"type\": \"string\"},\n",
    "    #                 \"demo_requested\": {\"type\": \"boolean\"}\n",
    "    #             },\n",
    "    #             \"required\": [\"name\", \"email\", \"plan_interest\", \"demo_requested\"],\n",
    "    #             \"additionalProperties\": False\n",
    "    #         }\n",
    "    #     }\n",
    ")\n",
    "\n",
    "# event = response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-D5hVsdehwrMt0EWzNSQwTKJlrygGr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Great question! In Python, â€œbestâ€ depends on what youâ€™re trying to do. Hereâ€™s a quick, Pythonic guide with patterns to choose from:\\n\\n- Iterate over items (default)\\n  for x in items:\\n      ...\\n\\n- Need index and value\\n  for i, x in enumerate(items):  # optionally enumerate(items, start=1)\\n      ...\\n\\n- Iterate two (or more) sequences together\\n  for a, b in zip(a_list, b_list):\\n      ...\\n\\n- Fixed number of times\\n  for _ in range(n):\\n      ...\\n\\n- Loop until a condition changes\\n  while condition:\\n      ...\\n\\n- Build a new collection (prefer comprehensions)\\n  squares = [x*x for x in nums]              # list\\n  evens = {x for x in nums if x % 2 == 0}    # set\\n  mapping = {x: x*x for x in nums}           # dict\\n\\n- Stream values without making a list (generator)\\n  total = sum(x*x for x in nums if x > 0)\\n\\n- Search with clean â€œnot foundâ€ handling (forâ€“else)\\n  for user in users:\\n      if user.id == target:\\n          found = user\\n          break\\n  else:\\n      found = None\\n\\n- Iterate dictionaries correctly\\n  for key, value in my_dict.items():\\n      ...\\n\\n- Reverse, sorted, or with custom order\\n  for x in reversed(seq): ...\\n  for k in sorted(my_dict): ...\\n\\nBest practices and tips\\n- Prefer iterating directly over items; avoid range(len(seq)) unless you truly need indices.\\n- Use clear variable names and small loop bodies; extract helper functions if logic grows.\\n- Donâ€™t mutate a list youâ€™re iterating. Either iterate over a copy (for x in list(lst):) or build a new list.\\n- Use built-ins for aggregates: sum, min, max, any, all with generator expressions.\\n- Avoid string concatenation in a loop; accumulate and ''.join(parts) at the end.\\n- For performance-heavy numeric work, consider NumPy/Pandas (vectorization).\\n- Explore itertools (chain, islice, groupby, accumulate) for efficient, readable pipelines.\\n\\nCommon mini-patterns\\n- Count things: from collections import Counter; counts = Counter(items)\\n- Pair neighbors: for a, b in zip(items, items[1:]): ...\\n- Index starting at 1: for i, x in enumerate(items, 1): ...\\n\\nIf you share what youâ€™re looping over and what outcome you want (transform, filter, search, side effects, early exit, etc.), I can suggest the most fitting pattern with a tailored example.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1770251276, model='gpt-5-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1708, prompt_tokens=35, total_tokens=1743, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=1152, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2023', '2024', '2020', '2025'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
    "chat_openai = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_openai.run([{'role': 'user', 'content': 'say hello'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2023',\n",
       "  'use of this year. Success, and failure, are lagging indicators. Mountain \\nclimbing disasters are always a series of small, seemingly inconsequential decisions that interact in unexpected \\nways, compounding exponentially. Success works the same way. \\nIt is fashionable these days to say investment outcomes follow a Power Law. Thatâ€™s technically true and soulless. At \\nStone Ridge, the compounding weâ€™re after is in wisdom and trust in our relationships with each other. Life is much \\nmore satisfying when we realize that relationships also follow a Power Law, and invest accordingly.\\nSecond, we let the simplicity of our business model be enough. The humility of our questions and the plainness of our \\nactions â€“ our undefended openness â€“ may occasionally seem to border on irresponsible naivete, but we rarely cross \\nthat border. And even when we do, we have each otherâ€™s backs with such ferocity that we never fail to pull ourselves \\nback to safety. \\nOur most sustainable competitive advantage may ',\n",
       "  np.float32(0.12062941)),\n",
       " ('2023',\n",
       "  'un)\\ncorrelation. However, even conditional on all of that, we still have one final, highly strategic, highly consequential \\ndecision: risk selection. That is, how much risk are we comfortable taking and, given that, how specifically shall \\nwe participate in the industry? \\nInternally, and perhaps too quantitatively, we call our approach â€œThe Three Little Bearsâ€: not too hot, not too cold. \\nAlong the dimensions of risk that we care about â€“ market risk, regulatory risk, and reputational risk â€“ â€œtoo coldâ€ is \\ntoo safe and not enough expected return. â€œToo hotâ€ has high return potential, but too much uncertainty for our taste, \\nparticularly around known and unknown unknowns in stressed markets. We seek â€œjust rightâ€ for us. \\nIn alternative lending, this means no AAA student loans (too cold) and no subprime or Emerging Markets (too \\nhot). Prime, U.S.-only is just right. In SFR, this means no homes over $500k (too cold) or under $150k (too hot). It \\nalso means no low-growth Detroit (too cold) a',\n",
       "  np.float32(0.12062941)),\n",
       " ('2024',\n",
       "  'uring the war, occasionally resulted in \\nstray food â€“ cargo â€“ accidentally falling out of the back of airplanes into their territory, perhaps those planes would \\nreturn and drop cargo again.\\nAnthropologists coined the term â€œcargo cultâ€ to mean effort designed to create a specific outcome that has no \\nrelationship to the output of the effort.\\nFiat central bankers observe that wealthy people have money, so they reason that printing more money \\nwill make more people wealthy.  Printing pieces of paper to attract prosperity is no less preposterous than \\nbuilding a wooden bird to attract an airplane.  \\nPrinting paper money is like a stock split.  Nothing changes about the quality of a companyâ€™s products or its \\nclient relationships when the price halves and the share count doubles overnight via split.  Just as one share of \\npost-split stock represents less of a claim on the company than it did the day prior, immediately after a fresh fiat \\ncounterfeiting printing, one unit of paper money rep',\n",
       "  np.float32(0.12062941)),\n",
       " ('2024',\n",
       "  'uake destroyed 3,400 acres of buildings in the heart of the city, including half the \\nstructures that housed its population of ~400,000, triggering the greatest housing challenge in US history.\\nSome people immediately left the city to stay with relatives nearby.  For others, camps and shelters were quickly \\nestablished and new construction proceeded rapidly.  Yet, about one-fifth of San Franciscoâ€™s population had to fit, \\nsomehow, into the still-standing half of the housing stock, which was already occupied.  This meant that each \\nremaining house had to shelter, somehow, on average 40% more people, and fast. \\nNot only was there no subsequent homeless problem, the first post-quake publication of the San Francisco Chronicle, \\nfive weeks post-event, did not even mention a housing shortage.  The classifieds contained 64 advertisements of \\nhouses â€œavailable for rent,â€ 19 â€œfor sale,â€ and only five for â€œwanted to rentâ€.  \\nFast forward a mere 40 years, to 1946, just post-war.  The San Francisc',\n",
       "  np.float32(0.12062941))]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.search_by_text('what is the best part of waking up', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
