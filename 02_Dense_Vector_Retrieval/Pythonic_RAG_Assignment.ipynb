{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lElF3o5PR6ys"
   },
   "source": [
    "# Your First RAG Application: Stone Ridge Investor Letter Assistant\n",
    "\n",
    "> **Note:** While the examples in this notebook use the OpenAI API, please follow the best practices outlined in the [SRHG AI Usage Guidelines](https://srhg.enterprise.slack.com/docs/T0HANKTEC/F0AB86J3A1L).\n",
    "\n",
    "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application by building a **Stone Ridge Investor Letter Assistant**.\n",
    "\n",
    "Imagine having an AI assistant that can answer your questions about Stone Ridge's investment philosophy, market insights, and strategic outlook based on their investor letters - that's exactly what we'll build here. We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
    "\n",
    "> NOTE: This was done with Python 3.12.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CtcL8P8R6yt"
   },
   "source": [
    "## Table of Contents:\n",
    "\n",
    "- Task 1: Imports and Utilities\n",
    "- Task 2: Documents (Loading the Stone Ridge Investor Letter)\n",
    "- Task 3: Embeddings and Vectors\n",
    "- Task 4: Prompts\n",
    "- Task 5: Retrieval Augmented Generation (Building the Investor Letter Assistant)\n",
    "  - ðŸš§ Activity #1: Enhance Your Investor Letter Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Dz6GYilR6yt"
   },
   "source": [
    "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
    "\n",
    "<img src=\"https://i.imgur.com/vD8b016.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjmC0KFtR6yt"
   },
   "source": [
    "## Task 1: Imports and Utilities\n",
    "\n",
    "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z1dyrG4hR6yt"
   },
   "outputs": [],
   "source": [
    "from aimakerspace.text_utils import PDFFileLoader, CharacterTextSplitter\n",
    "from aimakerspace.vectordatabase import VectorDatabase\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9OrFZRnER6yt"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0jGnpQsR6yu"
   },
   "source": [
    "## Task 2: Documents\n",
    "\n",
    "We'll be concerning ourselves with this part of the flow in the following section:\n",
    "\n",
    "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SFPWvRUR6yu"
   },
   "source": [
    "### Loading Source Documents\n",
    "\n",
    "So, first things first, we need some documents to work with.\n",
    "\n",
    "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
    "\n",
    "In this case, we're going to parse our PDF file into a single document in memory.\n",
    "\n",
    "Let's look at the relevant bits of the `PDFFileLoader` class:\n",
    "\n",
    "```python\n",
    "def load_file(self):\n",
    "        doc = pymupdf.open(self.path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        self.documents.append(text)\n",
    "```\n",
    "\n",
    "We're loading the PDF document using PyMuPDF, extracting text from each page, and storing that output in our `self.documents` list.\n",
    "\n",
    "> NOTE: We're using the Stone Ridge 2025 Investor Letter as our sample data. This content covers investment philosophy, market analysis, and strategic insights - perfect for building an investor letter assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ia2sUEuGR6yu",
    "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader = PDFFileLoader(\"data/Stone Ridge 2025 Investor Letter.pdf\")\n",
    "documents = pdf_loader.load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV-tj5WFR6yu",
    "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025 Investor Letter\n",
      "Investor Letter\n",
      "â€œEvery driver has a limit.  Mine is a little bit further than o\n"
     ]
    }
   ],
   "source": [
    "print(documents[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHlTvCzYR6yu"
   },
   "source": [
    "### Splitting Text Into Chunks\n",
    "\n",
    "As we can see, there is one massive document.\n",
    "\n",
    "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
    "\n",
    "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
    "\n",
    "For this toy example, we'll just split blindly on length.\n",
    "\n",
    ">There's an opportunity to clear up some terminology here, for this course we will stick to the following:\n",
    ">\n",
    ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
    ">- \"document(s)\" : single (or more) text object(s)\n",
    ">- \"corpus\" : the combination of all of our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6Voc0jR6yv"
   },
   "source": [
    "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into manageable sized chunks that retain the most relevant local context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMC4tsEmR6yv",
    "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "split_documents = text_splitter.split_texts(documents)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a section to tag whats endnotes and whats not\n",
    "new_split_docs = []\n",
    "section = 'body'\n",
    "for doc in split_documents:\n",
    "    if 'Endnotes' in doc:\n",
    "        pre, post = doc.split('Endnotes')\n",
    "        if len(pre):\n",
    "            new_split_docs.append((pre, 'body'))\n",
    "        if len(post):\n",
    "            new_split_docs.append((post, 'endnotes'))\n",
    "\n",
    "        section = 'endnotes'\n",
    "    else:\n",
    "        new_split_docs.append((doc, section))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2wKT0WLR6yv"
   },
   "source": [
    "Let's take a look at some of the documents we've managed to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcYMwWJoR6yv",
    "outputId": "20d69876-feca-4826-b4be-32915276987a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2025 Investor Letter\\nInvestor Letter\\nâ€œEvery driver has a limit.  Mine is a little bit further than others.â€\\nâ€”\\u2002 Ayrton Senna, greatest Formula One driver of all time\\nâ€œIâ€™m not funny.  What I am is brave.â€\\nâ€”\\u2002 Lucille Ball, greatest female comedian of all time\\nâ€œIâ€™d rather be optimistic and wrong than pessimistic and right.â€\\nâ€”\\u2002 Elon Musk\\nâ€œI can outlearn you.  \\nI can outread you.  \\nI can outthink you.  \\nAnd I can outphilosophize you.â€\\nâ€”\\u2002 Robert De Niro as Max Cady, Cape Fear\\nâ€œIâ€™m not superstitious, but I am a little stitious.â€\\nâ€”\\u2002 Michael Scott, The Office, on how Bayesians form priors \\nâ€œA wall is a very big weapon.â€\\nâ€”\\u2002 Banksy\\nDecember 2025\\nDear Fellow Investor,\\nThe greatest treasure hunters know that the treasure they seek is not lost.  It is waiting.\\nStarting in 1969 and for 5,954 straight days, treasure hunter Mel Fisher sought what was waiting for him: the \\nwreckage of the Nuestra SeÃ±ora de Atocha, a Spanish galleon that sunk in 1622.  \\nHow do you recruit, retain, and motivate a crew when',\n",
       "  'body')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_split_docs[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tion set \\nforth herein. By accepting this communication, the recipient acknowledges its understanding and acceptance of the foregoing \\nterms.\\n',\n",
       "  'endnotes')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_split_docs[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOU-RFP_R6yv"
   },
   "source": [
    "## Task 3: Embeddings and Vectors\n",
    "\n",
    "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
    "\n",
    "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API Key\n",
    "\n",
    "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
    "\n",
    "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "\n",
    "Let's set up our vector database to hold all our documents and their embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDQrfAR1R6yv"
   },
   "source": [
    "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
    "\n",
    "Let's look at our `VectorDatabase().__init__()`:\n",
    "\n",
    "```python\n",
    "def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors = defaultdict(np.array)\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "```\n",
    "\n",
    "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
    "\n",
    "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
    "\n",
    "> **Quick Info About `text-embedding-3-small`**:\n",
    "> - It has a context window of **8191** tokens\n",
    "> - It returns vectors with dimension **1536**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L273pRdeR6yv"
   },
   "source": [
    "#### â“Question #1:\n",
    "\n",
    "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
    "\n",
    "1. Is there any way to modify this dimension?\n",
    "2. What technique does OpenAI use to achieve this?\n",
    "\n",
    "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1.1, and [this paper](https://arxiv.org/abs/2205.13147) for an answer to question #1.2!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "1. You can pass in a different value for the \"dimensions\" parameter to the request body\n",
    "2. Open AI uses a method called Matryoshka Reprsentation Learning where the embeddings created are increasing in granularity. The idea being if you took the first N dimensions, that should be comparable to embedding in N dimensions, each incremental dimension adds to the granularity of the embedding but does not reflect a change to its previous embeddings. This is sort of like a decimal, where you can truncate the last n digits without losing any of the information in the first n digits. By including more digits, or dimensions, you only get to a more granular representation of the information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5FZY7K3R6yv"
   },
   "source": [
    "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
    "\n",
    "```python\n",
    "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        return await aget_embeddings(\n",
    "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSct6X0aR6yv"
   },
   "source": [
    "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
    "\n",
    "```python\n",
    "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
    "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
    "        for text, embedding in zip(list_of_text, embeddings):\n",
    "            self.insert(text, np.array(embedding))\n",
    "        return self\n",
    "```\n",
    "\n",
    "And that's all we need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "O4KoLbVDR6yv"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aee33b31a8b492cba6fad1051da70f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from aimakerspace.vectordatabase import *\n",
    "\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(list_of_text)\n",
    "        return list(embeddings)\n",
    "\n",
    "    async def async_get_embedding(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(list_of_text)\n",
    "        return list(embeddings)\n",
    "\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text)\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors = defaultdict(np.array)\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "\n",
    "    def insert(self, key: str, vector: np.array) -> None:\n",
    "        self.vectors[key] = vector\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query_vector: np.array,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        scores = [\n",
    "            (key, distance_measure(query_vector, vector))\n",
    "            for key, vector in self.vectors.items()\n",
    "        ]\n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    def search_by_text(\n",
    "        self,\n",
    "        query_text: str,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "        return_as_text: bool = False,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        query_vector = self.embedding_model.get_embedding(query_text)\n",
    "        results = self.search(query_vector, k, distance_measure)\n",
    "        return [result[0] for result in results] if return_as_text else results\n",
    "\n",
    "    def retrieve_from_key(self, key: str) -> np.array:\n",
    "        return self.vectors.get(key, None)\n",
    "\n",
    "    async def abuild_from_list(self, list_of_text: List[Tuple[str, str]]) -> \"VectorDatabase\":\n",
    "        embeddings = await self.embedding_model.async_get_embeddings([t[0] for t in list_of_text])\n",
    "        for (text, metadata), embedding in zip(list_of_text, embeddings):\n",
    "            if metadata == 'body':\n",
    "                self.insert(text, np.array(embedding))\n",
    "        return self\n",
    "\n",
    "em = EmbeddingModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = VectorDatabase(em)\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(new_split_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSZwaGvpR6yv"
   },
   "source": [
    "#### â“Question #2:\n",
    "\n",
    "What are the benefits of using an `async` approach to collecting our embeddings?\n",
    "\n",
    "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "Since this is presumably IO bound, using async means you can send all your requests in one shot (or a few batches) and wait for the responses, this allows us to lean on any horizontal scaling in the openai embedding api. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRBdIt-xR6yw"
   },
   "source": [
    "So, to review what we've done so far in natural language:\n",
    "\n",
    "1. We load source documents\n",
    "2. We split those source documents into smaller chunks (documents)\n",
    "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
    "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-vWANZyR6yw"
   },
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
    "\n",
    "We're going to use the following process to achieve this in our toy example:\n",
    "\n",
    "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
    "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
    "3. We return a list of the top `k` closest vectors, with their text representations\n",
    "\n",
    "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
    "\n",
    "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
    "\n",
    "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hen we believe \\nit is time â€“ have been the hallmarks of our operating model since firm inception. \\nThe first rule of product design at Stone Ridge is we build products we want for ourselves.  Stone Ridge is always \\nthe seed investor and, whenever possible â€“ but not yet always â€“ the largest.  Consider our â€œno-deviceâ€ policy in \\nthis context.  Would you ever pull out your phone and start scrolling in the middle of a meeting with your largest \\ninvestor?  At Stone Ridge, we do not pull our phones out on each other.\\nWith Stone Ridge increasingly Stone Ridgeâ€™s largest investor, the scale of our profits as principals, not agents, creates \\na level of alignment with our outside investors we are unaware exists elsewhere.  Those investors occasionally \\nremark that this is a wonderfully clarifying and palpable distinction, and that Stone Ridge feels different inside. \\nEarlier this year, Stone Ridge Holdings Group (SRHG), parent company of Stone Ridge Asset Management, NYDIG, \\nand Longtail Re, comp',\n",
       "  np.float32(0.62319523)),\n",
       " ('ave received 32x their original investment in distributions and now own equity \\nvalued up 178x net of those distributions, an annualized total return of 50%.  Fisher took 5,954 days to find the \\nAtocha.  We have been at it for 4,930.  Every morning, I bang our little shipâ€™s pots and pans with a smile because \\nI know Stone Ridge will find the treasure. \\nTodayâ€™s the day.\\n*****\\nOUR PARTNERSHIP\\nAs we enter 2026, our tanks are filled with energy, gratitude, and inspiration.  We innovate to prepare for an \\nuncertain future, in pursuit of our mission: financial security for all.\\nStone Ridge is most proud of the partnership we have with you, our investors.  We are on the path together.  You \\ncontribute the capital to propel and sustain groundbreaking product development.  We contribute our collective \\ncareersâ€™ worth of experience in sourcing, structuring, execution, and risk management.  Together, it works.  In that \\nspirit, I offer my deepest gratitude to you for sharing your wealth with us t',\n",
       "  np.float32(0.56189543)),\n",
       " (' palpable distinction, and that Stone Ridge feels different inside. \\nEarlier this year, Stone Ridge Holdings Group (SRHG), parent company of Stone Ridge Asset Management, NYDIG, \\nand Longtail Re, completed a significant primary equity financing led by an iconic American financial services firm \\nâ€“ also a mutual, though this one a relatively spry 165-year-old â€“ we are thrilled to partner with in the decades ahead. \\nDespite regularly rising profitability, regularly raising primary equity for our parent and affiliates â€“ $4.6 billion \\nand counting, and only primary, ever â€“ has been our quiet way of strengthening our most important partnerships, \\nplaying muscular defense when the sky falls in our markets, and playing profitable post-event offense in the \\naftermath.\\nFounding 2012 SRHG investors have received 32x their original investment in distributions and now own equity \\nvalued up 178x net of those distributions, an annualized total return of 50%.  Fisher took 5,954 days to find the \\nAtoch',\n",
       "  np.float32(0.5543321))]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.search_by_text(\"What is Stone Ridge's investment philosophy?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TehsfIiKR6yw"
   },
   "source": [
    "## Task 4: Prompts\n",
    "\n",
    "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
    "\n",
    "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
    "\n",
    "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXpA0UveR6yw"
   },
   "source": [
    "### XYZRolePrompt\n",
    "\n",
    "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
    "\n",
    "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
    "\n",
    "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
    "\n",
    "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the modelâ€™s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
    "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
    "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
    "\n",
    "The main idea is this:\n",
    "\n",
    "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
    "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
    "3. Then, you prompt the model with the true \"user\" message.\n",
    "\n",
    "In this example, we'll be forgoing the 2nd step for simplicity's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdZ2KWKSR6yw"
   },
   "source": [
    "#### Utility Functions\n",
    "\n",
    "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFbeJDDsR6yw"
   },
   "source": [
    "##### XYZRolePrompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mojJSE3R6yw"
   },
   "source": [
    "Here we have our `system`, `user`, and `assistant` role prompts.\n",
    "\n",
    "Let's take a peek at what they look like:\n",
    "\n",
    "```python\n",
    "class BasePrompt:\n",
    "    def __init__(self, prompt):\n",
    "        \"\"\"\n",
    "        Initializes the BasePrompt object with a prompt template.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        \"\"\"\n",
    "        self.prompt = prompt\n",
    "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
    "\n",
    "    def format_prompt(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Formats the prompt string using the keyword arguments provided.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: The formatted prompt string\n",
    "        \"\"\"\n",
    "        matches = self._pattern.findall(self.prompt)\n",
    "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
    "\n",
    "    def get_input_variables(self):\n",
    "        \"\"\"\n",
    "        Gets the list of input variable names from the prompt string.\n",
    "\n",
    "        :return: List of input variable names\n",
    "        \"\"\"\n",
    "        return self._pattern.findall(self.prompt)\n",
    "```\n",
    "\n",
    "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
    "\n",
    "```python\n",
    "class RolePrompt(BasePrompt):\n",
    "    def __init__(self, prompt, role: str):\n",
    "        \"\"\"\n",
    "        Initializes the RolePrompt object with a prompt template and a role.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
    "        \"\"\"\n",
    "        super().__init__(prompt)\n",
    "        self.role = role\n",
    "\n",
    "    def create_message(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a message dictionary with a role and a formatted message.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: Dictionary containing the role and the formatted message\n",
    "        \"\"\"\n",
    "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
    "```\n",
    "\n",
    "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
    "\n",
    "```python\n",
    "class SystemRolePrompt(RolePrompt):\n",
    "    def __init__(self, prompt: str):\n",
    "        super().__init__(prompt, \"system\")\n",
    "```\n",
    "\n",
    "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D361R6sMR6yw"
   },
   "source": [
    "##### ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJVQ2Pm8R6yw"
   },
   "source": [
    "Next we have our model, which is converted to a format analogous to libraries like LangChain and LlamaIndex.\n",
    "\n",
    "Let's take a peek at how that is constructed:\n",
    "\n",
    "```python\n",
    "class ChatOpenAI:\n",
    "    def __init__(self, model_name: str = \"gpt-4.1-mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if self.openai_api_key is None:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "    def run(self, messages, text_only: bool = True):\n",
    "        if not isinstance(messages, list):\n",
    "            raise ValueError(\"messages must be a list\")\n",
    "\n",
    "        openai.api_key = self.openai_api_key\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name, messages=messages\n",
    "        )\n",
    "\n",
    "        if text_only:\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        return response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCU7FfhIR6yw"
   },
   "source": [
    "#### â“ Question #3:\n",
    "\n",
    "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
    "\n",
    "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "You can use the instructions api to pass in instructions that you may otherwise put in the developer role message. I guess this is theoretically useful if you want to keep a conversation going and only want to apply instructions to certain messages, i wonder if you removed the developer message on those conversation lines if it would confuse the llm. Not clear how useful this is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5wcjMLCR6yw"
   },
   "source": [
    "### Creating and Prompting OpenAI's `gpt-4.1-mini`!\n",
    "\n",
    "Let's tie all these together and use it to prompt `gpt-4.1-mini`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "WIfpIot7R6yw"
   },
   "outputs": [],
   "source": [
    "from aimakerspace.openai_utils.prompts import (\n",
    "    UserRolePrompt,\n",
    "    SystemRolePrompt,\n",
    "    AssistantRolePrompt,\n",
    ")\n",
    "\n",
    "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
    "\n",
    "chat_openai = ChatOpenAI()\n",
    "user_prompt_template = \"{content}\"\n",
    "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
    "system_prompt_template = (\n",
    "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
    ")\n",
    "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
    "\n",
    "messages = [\n",
    "    system_role_prompt.create_message(expertise=\"Python\"),\n",
    "    user_role_prompt.create_message(\n",
    "        content=\"What is the best way to write a loop?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = chat_openai.run(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHo7lssNR6yw",
    "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! The \"best\" way to write a loop in Python depends on what you're trying to achieve, but generally, Python's `for` loops and `while` loops are both very useful and have their ideal use cases.\n",
      "\n",
      "Hereâ€™s a brief overview to help you write effective loops:\n",
      "\n",
      "### Using a `for` loop\n",
      "`for` loops are great when you want to iterate over a sequence (like a list, tuple, string, or range).\n",
      "\n",
      "Example:\n",
      "```python\n",
      "# Looping through a list\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "\n",
      "If you want to loop a specific number of times:\n",
      "```python\n",
      "for i in range(5):\n",
      "    print(i)\n",
      "```\n",
      "\n",
      "### Using a `while` loop\n",
      "`while` loops are useful when you want to repeat something until a condition changes.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "count = 0\n",
      "while count < 5:\n",
      "    print(count)\n",
      "    count += 1\n",
      "```\n",
      "\n",
      "### Best Practices\n",
      "- Prefer `for` loops when iterating over collections or a known number of iterations.\n",
      "- Use `while` loops when the number of repetitions depends on a condition changing within the loop.\n",
      "- Make sure to update the loop variable or condition within `while` loops to avoid infinite loops.\n",
      "- Keep the loop body as simple as possible.\n",
      "- Use descriptive variable names for readability.\n",
      "\n",
      "If you have a specific task or type of loop in mind, feel free to share it, and Iâ€™d be happy to help you write it! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2nxxhB2R6yy"
   },
   "source": [
    "## Task 5: Retrieval Augmented Generation\n",
    "\n",
    "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
    "\n",
    "There is much you could do here, many tweaks and improvements to be made!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "D1hamzGaR6yy"
   },
   "outputs": [],
   "source": [
    "RAG_SYSTEM_TEMPLATE = \"\"\"You are a helpful investor letter assistant that answers questions about Stone Ridge's investment philosophy, market insights, and strategic outlook based strictly on provided context.\n",
    "\n",
    "Instructions:\n",
    "- Only answer questions using information from the provided context\n",
    "- If the context doesn't contain relevant information, respond with \"I don't have information about that in the investor letter\"\n",
    "- Be accurate and cite specific parts of the context when possible\n",
    "- Keep responses {response_style} and {response_length}\n",
    "- Only use the provided context. Do not use external knowledge.\n",
    "- Include a reminder that this is for informational purposes only and not investment advice when appropriate\n",
    "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
    "\n",
    "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
    "{context}\n",
    "\n",
    "Number of relevant sources found: {context_count}\n",
    "{similarity_scores}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Please provide your answer based solely on the context above.\"\"\"\n",
    "\n",
    "rag_system_prompt = SystemRolePrompt(\n",
    "    RAG_SYSTEM_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"response_style\": \"concise\",\n",
    "        \"response_length\": \"brief\"\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_user_prompt = UserRolePrompt(\n",
    "    RAG_USER_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"context_count\": \"\",\n",
    "        \"similarity_scores\": \"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI, vector_db_retriever: VectorDatabase, \n",
    "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.response_style = response_style\n",
    "        self.include_scores = include_scores\n",
    "\n",
    "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
    "        # Retrieve relevant contexts\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
    "        \n",
    "        context_prompt = \"\"\n",
    "        similarity_scores = []\n",
    "        \n",
    "        for i, (context, score) in enumerate(context_list, 1):\n",
    "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
    "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
    "        \n",
    "        # Create system message with parameters\n",
    "        system_params = {\n",
    "            \"response_style\": self.response_style,\n",
    "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
    "        }\n",
    "        \n",
    "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
    "        \n",
    "        user_params = {\n",
    "            \"user_query\": user_query,\n",
    "            \"context\": context_prompt.strip(),\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
    "        }\n",
    "        \n",
    "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
    "\n",
    "        return {\n",
    "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
    "            \"context\": context_list,\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
    "            \"prompts_used\": {\n",
    "                \"system\": formatted_system_prompt,\n",
    "                \"user\": formatted_user_prompt\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the provided excerpts from the investor letter, several key investment themes emerge:\n",
      "\n",
      "1. **Pursuit of Competitive Advantage Through Sophistication and Operational Excellence**  \n",
      "   The letter discusses Stone Ridge's initial belief that breakthroughs in proprietary financing and hedging technology, resulting in an edge in cost of funds, would suffice for best-in-class investment performance in energy markets (Source 1). However, they learned that financial sophistication alone is insufficient without industrial control and operational expertise. This led them to partner with Flywheel Energy, recognized for superior operations partly due to their leadership's elite military backgrounds. This highlights a theme of combining financial innovation with operational mastery to achieve superior investment outcomes.\n",
      "\n",
      "2. **Long-Term Perseverance and Search for Value**  \n",
      "   Stone Ridge draws an analogy to the long search for the treasure of the Atocha shipwreck, emphasizing patience and persistence over thousands of days (Source 2). This metaphor reflects a commitment to disciplined, long-term investing with a focus on discovering undervalued opportunities that might require sustained effort.\n",
      "\n",
      "3. **Partnership and Shared Mission with Investors**  \n",
      "   The firm expresses gratitude toward its investors and emphasizes the partnership ethos: investors provide capital to fuel innovation and product development, while Stone Ridge contributes deep expertise in sourcing, structuring, execution, and risk management (Source 2). This theme showcases a collaborative approach where aligned incentives and shared dedication to financial security underpin their operations.\n",
      "\n",
      "4. **Risk Management and Selective Investment Approach in Complex Markets**  \n",
      "   In their reinsurance business (Longtail Re), initiated in 2020, Stone Ridge focuses on partnering with top underwriters to build a hyper-diversified portfolio of casualty liabilities that generate low-cost float, which is then deployed into proprietary fixed income strategies (Source 3). They are cautious of adverse selection and highlight the importance of selectivity and avoiding superficially attractive but risky trades. The firm underscores its philosophy of prudent risk transfer pricing and strategic partnerships rather than competing with established legacy underwriters.\n",
      "\n",
      "5. **Innovation Amidst Uncertainty**  \n",
      "   The letter communicates a forward-looking theme of innovating to prepare for an uncertain future and pursuing the mission of financial security for all (Source 2). This suggests a broad investment philosophy oriented towards adaptive strategies and continuous product development in the face of changing market and economic conditions.\n",
      "\n",
      "In summary, the key investment themes include leveraging proprietary financial and operational advantages, long-term value discovery with persistence, strong partnership alignment with investors, disciplined and selective risk management especially in insurance-related markets, and ongoing innovation to address uncertainty and secure financial outcomes.\n",
      "\n",
      "**Note:** This response is for informational purposes only and does not constitute investment advice.\n",
      "\n",
      "Context Count: 3\n",
      "Similarity Scores: ['Source 1: 0.434', 'Source 2: 0.432', 'Source 3: 0.410']\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai,\n",
    "    response_style=\"detailed\",\n",
    "    include_scores=True\n",
    ")\n",
    "\n",
    "result = rag_pipeline.run_pipeline(\n",
    "    \"What are the key investment themes discussed in the letter?\",\n",
    "    k=3,\n",
    "    response_length=\"comprehensive\", \n",
    "    include_warnings=True,\n",
    "    confidence_required=True\n",
    ")\n",
    "\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"\\nContext Count: {result['context_count']}\")\n",
    "print(f\"Similarity Scores: {result['similarity_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZIJI19uR6yz"
   },
   "source": [
    "#### â“ Question #4:\n",
    "\n",
    "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
    "\n",
    "What is that strategy called?\n",
    "\n",
    "> NOTE: You can look through our [OpenAI Responses API](https://colab.research.google.com/drive/14SCfRnp39N7aoOx8ZxadWb0hAqk4lQdL?usp=sharing) notebook for an answer to this question if you get stuck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "You could jack up the reasoning effort, the other thing would be using the developer role, but its not clear to me how this is different than the system role - they seem to be somwhat interchangeable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—ï¸ Activity #1:\n",
    "\n",
    "Enhance your Stone Ridge Investor Letter Assistant in some way! \n",
    "\n",
    "Suggestions are: \n",
    "\n",
    "- **Multi-Document Support**: Allow it to ingest multiple investor letters from different years for historical comparison\n",
    "- **New Distance Metric**: Implement a different similarity measure - does it improve retrieval for financial content?\n",
    "- **Metadata Support**: Add metadata like year, topic categories (market outlook, strategy, performance) to help filter results\n",
    "- **Different Embedding Model**: Try a different embedding model - does domain-specific tuning help for financial content?\n",
    "- **Multi-Source Ingestion**: Add the capability to ingest content from SEC filings, earnings calls, or other financial documents\n",
    "\n",
    "While these are suggestions, you should feel free to make whatever augmentations you desire! Think about what features would make this investor letter assistant most useful for understanding Stone Ridge's investment approach.\n",
    "\n",
    "When you're finished making the augments to your RAG application - vibe check it against the old one - see if you can \"feel the improvement\"!\n",
    "\n",
    "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!\n",
    "\n",
    "> NOTE: If you're not sure where to start - ask Cursor (CMD/CTRL+L) to guide you through the changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/2023 Stone Ridge Investor Letter No Front Image.pdf 2023\n",
      "2023 60\n",
      "./data/Stone_Ridge_2024_Investor_Letter.pdf 2024\n",
      "2024 86\n",
      "./data/2020 Stone Ridge Investor Letter No Front Image.pdf 2020\n",
      "2020 75\n",
      "./data/Stone Ridge 2025 Investor Letter.pdf 2025\n",
      "2025 56\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "docs_by_year = {}\n",
    "for fname in os.listdir('./data'):\n",
    "    year = str(int(re.findall(r'\\d+', fname)[0]))\n",
    "    print(os.path.join('./data', fname), year)\n",
    "    pdf_loader = PDFFileLoader(os.path.join('./data', fname))\n",
    "    documents = pdf_loader.load_documents()\n",
    "    assert len(documents) == 1\n",
    "    \n",
    "    assert 'Endnotes' in documents[0]\n",
    "    relevant_doc = documents[0].split('Endnotes')[0]\n",
    "    # some cleaning\n",
    "    relevant_doc = ''.join(e if e.isprintable() else ' 'for e in relevant_doc)\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    split_documents = text_splitter.split_texts([relevant_doc])\n",
    "    docs_by_year[year] = split_documents\n",
    "    print(year, len(split_documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4173327cf20545289feb2f1444add469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from aimakerspace.vectordatabase import *\n",
    "\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(list_of_text)\n",
    "        return list(embeddings)\n",
    "\n",
    "    async def async_get_embedding(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(list_of_text)\n",
    "        return list(embeddings)\n",
    "\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text)\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors_by_year = defaultdict(lambda: defaultdict(np.array))\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "\n",
    "    def insert(self, year: str, key: str, vector: np.array) -> None:\n",
    "        self.vectors_by_year[year][key] = vector\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query_vector: np.array,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "        year = None\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        scores = [\n",
    "            (year, key, distance_measure(query_vector, vector))\n",
    "            for year, key_vector in self.vectors_by_year.items() for key, vector in key_vector.items()\n",
    "        ]\n",
    "        if year is not None:\n",
    "            scores = [a for a in scores if a[0] == year]\n",
    "        return sorted(scores, key=lambda x: x[2], reverse=True)[:k]\n",
    "\n",
    "    def search_by_text(\n",
    "        self,\n",
    "        query_text: str,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "        return_as_text: bool = False,\n",
    "        year = None,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        query_vector = self.embedding_model.get_embedding(query_text)\n",
    "        results = self.search(query_vector, k, distance_measure, year=year)\n",
    "        return [result[1] for result in results] if return_as_text else results\n",
    "\n",
    "    def retrieve_from_key(self, year, key: str) -> np.array:\n",
    "        return self.vectors.get(year, {}).get(key, None)\n",
    "\n",
    "    async def abuild_from_list(self, list_of_text_by_year: Dict[str, List[str]]) -> \"VectorDatabase\":\n",
    "        for year, list_of_text in list_of_text_by_year.items():\n",
    "            embeddings = await self.embedding_model.async_get_embeddings([t[0] for t in list_of_text])\n",
    "            for text, embedding in zip(list_of_text, embeddings):\n",
    "                self.insert(year, text, np.array(embedding))\n",
    "        return self\n",
    "\n",
    "em = EmbeddingModel()\n",
    "vector_db = VectorDatabase(em)\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(docs_by_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question: what were the key highlights of the 2020 investor letter\n",
      "Query for Vector DB: key highlights of the Stone Ridge investor letter from 2020\n",
      "Years to Query: ['2020']\n",
      "\n",
      "Q: What were the key themes in 2023 and 2024?\n",
      "Query: What were the key themes discussed in the Stone Ridge investor letters for the years 2023 and 2024?\n",
      "Years: ['2023', '2024']\n",
      "\n",
      "Q: Tell me about Stone Ridge's investment philosophy\n",
      "Query: What is Stone Ridge's investment philosophy?\n",
      "Years: ['all_years']\n",
      "\n",
      "Q: What did they say in the 2019 letter?\n",
      "Query: Information on the Stone Ridge investor letter from 2019.\n",
      "Years: ['invalid_year']\n",
      "\n",
      "Q: How did performance look in 2025?\n",
      "Query: Performance overview for the year 2025.\n",
      "Years: ['2025']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EDIT HERE AI!\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "year_options = ', '.join([f\"'{y}'\" for y in vector_db.vectors_by_year.keys()])\n",
    "# Define the structured output model\n",
    "class QueryResponseGetter(BaseModel):\n",
    "    years_to_query: List[str] = Field(\n",
    "        description=f\"Years to query - can be 'all_years', any combination of {year_options}, or 'invalid_year'\"\n",
    "    )\n",
    "    query: str = Field(\n",
    "        description=\"The search query to pass to the vector database\"\n",
    "    )\n",
    "\n",
    "def get_query_and_years(user_question: str) -> QueryResponseGetter:\n",
    "    \"\"\"\n",
    "    Given a user question, determine what query to use for vector search\n",
    "    and what years are relevant.\n",
    "    \"\"\"\n",
    "    available_years = list(vector_db.vectors_by_year.keys())\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model='gpt-4o-mini',  # Using OpenAI's model with structured outputs\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': f'''I have a vector database of Stone Ridge investor letters from the following years: {\", \".join(sorted(available_years))}.\n",
    "\n",
    "Based on the user question, you need to determine TWO things:\n",
    "1. What query should be used to search the vector database (reformulate if needed for better semantic search), each query to the vector db will only search a given year\n",
    "2. What years are relevant to answer this question\n",
    "\n",
    "For years_to_query, return:\n",
    "- [\"all_years\"] if the question is about general topics across all years\n",
    "- Specific years like [\"2020\"] or [\"2020\", \"2021\"] if the question asks about specific years\n",
    "- [\"invalid_year\"] if the question asks about years not in our database (e.g., 2019, 2026, etc.)\n",
    "\n",
    "Available years: {\", \".join(sorted(available_years))}'''\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': user_question\n",
    "            }\n",
    "        ],\n",
    "        response_format=QueryResponseGetter\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "# Test it out\n",
    "test_question = \"what were the key highlights of the 2020 investor letter\"\n",
    "result = get_query_and_years(test_question)\n",
    "print(f\"User Question: {test_question}\")\n",
    "print(f\"Query for Vector DB: {result.query}\")\n",
    "print(f\"Years to Query: {result.years_to_query}\")\n",
    "print()\n",
    "\n",
    "# Test with other questions\n",
    "test_questions = [\n",
    "    \"What were the key themes in 2023 and 2024?\",\n",
    "    \"Tell me about Stone Ridge's investment philosophy\",  # Should be all_years\n",
    "    \"What did they say in the 2019 letter?\",  # Should be invalid_year\n",
    "    \"How did performance look in 2025?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    result = get_query_and_years(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"Query: {result.query}\")\n",
    "    print(f\"Years: {result.years_to_query}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAugmentedQAPipelineColinVersion:\n",
    "    def __init__(self, llm: ChatOpenAI, vector_db_retriever: VectorDatabase, \n",
    "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.response_style = response_style\n",
    "        self.include_scores = include_scores\n",
    "\n",
    "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
    "        query_response = get_query_and_years(user_query)\n",
    "        if len(query_response.years_to_query) == 0 and query_response.years_to_query == \"invalid_year\":\n",
    "            response = self.llm.run([f\"create a short friendly response  telling them that the only investor letters avilable are: {year_options}\"])\n",
    "        else:\n",
    "            if len(query_response.years_to_query) == 0 and query_response.years_to_query == \"all_years\":\n",
    "                context_list = self.vector_db_retriever.search_by_text(query_response.query, k=k)\n",
    "            else:\n",
    "                context_list = []\n",
    "                for y in query_response.years_to_query:\n",
    "                    context_list += self.vector_db_retriever.search_by_text(user_query, k=k, year=y)\n",
    "        \n",
    "        context_prompt = \"\"\n",
    "        similarity_scores = []\n",
    "        \n",
    "        for i, (year, context, score) in enumerate(context_list, 1):\n",
    "            context_prompt += f\"[Source {i}]: (FROM {year} INVESTOR LETTER) {context}\\n\\n\"\n",
    "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
    "        \n",
    "        # Create system message with parameters\n",
    "        system_params = {\n",
    "            \"response_style\": self.response_style,\n",
    "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
    "        }\n",
    "        \n",
    "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
    "        \n",
    "        user_params = {\n",
    "            \"user_query\": user_query,\n",
    "            \"context\": context_prompt.strip(),\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
    "        }\n",
    "        \n",
    "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
    "\n",
    "        return {\n",
    "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
    "            \"context\": context_list,\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
    "            \"prompts_used\": {\n",
    "                \"system\": formatted_system_prompt,\n",
    "                \"user\": formatted_user_prompt\n",
    "            },\n",
    "            \"meta_prompt_results\": {\n",
    "                \"years\": query_response.years_to_query,\n",
    "                \"vector_query\": query_response.query\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Comparing the themes from Stone Ridgeâ€™s 2020 and 2023 investor letters reveals both continuity and evolution in their investment philosophy and market insights.\n",
      "\n",
      "2020 Themes:\n",
      "- The 2020 letters emphasize foundational macroeconomic insights, focusing on \"sound money\" and the deep systemic impacts of central banking policies, particularly the Federal Reserve's role since its founding in 1913. The letters critique modern central banking as a \"helicopter parent,\" distorting natural market Darwinism and economic price signals, which are crucial for proper coordination in the economy (Sources 1, 2, 3).\n",
      "- There is a pronounced concern about the unfairness and distortion caused by government policy money (GPM), its effect on inequality, and the resulting economic dislocations. The reflections include a philosophical tone about reconsidering what is \"real and essential,\" analogous to how fish relate to water (Source 1).\n",
      "- Monetary pluralism and its evolution over time are acknowledged as essential for civilizationâ€™s development (Source 2).\n",
      "- The 2020 letters contain skepticism about the prevailing monetary systemâ€™s sustainability and fairness, underscoring the long-term consequences of price signal degradation and wealth inequality triggered by monetary policy actions (Source 3).\n",
      "\n",
      "2023 Themes:\n",
      "- The 2023 letters lean more into operational reflections and strategic positioning, blending market realities with internal performance and decision-making. For instance, the firm highlights its ability to generate nearly $3 billion in uncorrelated trading profits with no down months, emphasizing perseverance and learning from prior setbacks such as failed fund launches or unexpected defaults among mining clients (Source 4).\n",
      "- There is a more granular focus on investment behavior dynamics, especially the distinction between principals investing their own money versus agents investing on behalf of others. The \"agent\" problem leads to suboptimal capital allocationâ€”agents avoid certain asset classes like reinsurance despite better risk-return profiles due to career risk concerns (Source 5).\n",
      "- The letters also explore nuanced asset class dynamics, giving art investment categories as an example: highlighting how newer categories (like Post-War and Contemporary art) command higher returns due to behavioral factors (billionaire wealth creation, desire for \"new new\" art) and risk-based asset pricing analogies (small-value stocks of art) (Source 6).\n",
      "- The 2023 letter acknowledges today's economic and investment complexity with a strategic lens, dealing with investor behavior, return patterns across vintages, and the impact of modern macroeconomic forces like globalization and central bank money printing.\n",
      "\n",
      "Summary Comparison:\n",
      "- The 2020 letters present a broad, deep macroeconomic and monetary theory critique highlighting systemic distortions and long-term structural risks.\n",
      "- The 2023 letters integrate those philosophical foundations with a sharper focus on practical investment execution, market realities, and behavioral finance insights affecting asset allocation and returns.\n",
      "- Both years share a critical perspective on central bank policies and distorted incentives but 2023 adds concrete examples of firm-level performance and detailed behavioral impediments in markets.\n",
      "- The evolution reflects a movement from understanding systemic macroeconomic forces to applying that understanding to asset allocation and investment strategy amid contemporary challenges.\n",
      "\n",
      "Note: This summary is exclusively based on the provided investor letter excerpts and should not be considered investment advice.\n",
      "\n",
      "Context Count: 6\n",
      "Similarity Scores: ['Source 1: 0.191', 'Source 2: 0.191', 'Source 3: 0.191', 'Source 4: 0.191', 'Source 5: 0.191', 'Source 6: 0.191']\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = RetrievalAugmentedQAPipelineColinVersion(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai,\n",
    "    response_style=\"detailed\",\n",
    "    include_scores=True\n",
    ")\n",
    "\n",
    "result = rag_pipeline.run_pipeline(\n",
    "    \"Compare themes from 2020 to 2023?\",\n",
    "    k=3,\n",
    "    response_length=\"comprehensive\", \n",
    "    include_warnings=True,\n",
    "    confidence_required=True\n",
    ")\n",
    "\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"\\nContext Count: {result['context_count']}\")\n",
    "print(f\"Similarity Scores: {result['similarity_scores']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'years': ['2020', '2023'],\n",
       " 'vector_query': 'Compare investment themes and market analyses presented in the investor letters from 2020 and 2023.'}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['meta_prompt_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
